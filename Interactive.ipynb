{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Govind Barbade\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import chromadb\n",
    "import json\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(content):\n",
    "    sentences = [sentence.strip() for sentence in content.split('.') if sentence.strip()]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'data.json'\n",
    "data = load_json(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = []\n",
    "metadata = []\n",
    "for entry in data:\n",
    "    content = entry['content']\n",
    "    url = entry['url']\n",
    "    entry_sentences = split_into_sentences(content)\n",
    "    sentences.extend(entry_sentences)\n",
    "    metadata.extend([url] * len(entry_sentences))\n",
    "\n",
    "document_dictionary = {str(i+1): sentence for i, sentence in enumerate(sentences)}\n",
    "print(len(document_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection_name = 'Nvidia_Docs'\n",
    "# client.create_collection(name=collection_name)\n",
    "\n",
    "collection = client.get_or_create_collection(name=collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cached = st.cache(allow_output_mutation=True)\n",
    "collection.add(\n",
    "    documents=[x for x in list(document_dictionary.values())],\n",
    "    metadatas=[{\"source\": metadata[i]} for i in range(len(metadata))],\n",
    "    ids=[str(x) for x in document_dictionary]\n",
    ")\n",
    "doc_count = collection.count()\n",
    "print(f\"Number of documents in the collection after addition: {doc_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_name = (\"deepset/roberta-base-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 13:45:18.794 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "if 'chat_history' not in st.session_state:\n",
    "    st.session_state.chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_container = st.container()\n",
    "with chat_container:\n",
    "    for chat in st.session_state.chat_history:\n",
    "        user_query, bot_response = chat['user'], chat['bot']\n",
    "        st.markdown(f\"\"\"\n",
    "        <div style='display: flex; justify-content: flex-end;'>\n",
    "            <div style='border: 1px solid #004d00; padding: 10px; border-radius: 10px; margin: 5px; background-color: #004d00; color: white;'>\n",
    "                <b>You:</b> {user_query}\n",
    "            </div>\n",
    "        </div>\n",
    "        <div style='display: flex; justify-content: flex-start;'>\n",
    "            <div style='border: 1px solid #001a00; padding: 10px; border-radius: 10px; margin: 5px; background-color: #001a00; color: white;'>\n",
    "                <b>Bot:</b> {bot_response}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    " \"Sorry, I couldn't find an answer to your question.\",\n",
    " \"Unfortunately, I couldn't locate an answer in the provided context.\",\n",
    " \"Apologies, I couldn't find relevant information to answer your question.\",\n",
    " \"I'm sorry, I couldn't find a suitable response in the given context.\",\n",
    " \"Regrettably, I couldn't extract an answer from the provided information.\",\n",
    " \"I apologize, I couldn't locate pertinent details to answer your question.\",\n",
    " \"Sorry, the question seems out of scope for the available information.\",\n",
    " \"Unfortunately, I couldn't determine a valid response given the context.\",\n",
    " \"I'm sorry, the question doesn't appear to match the available data.\",\n",
    " \"Apologies, I couldn't find a matching answer based on the provided context.\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Form Database: The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit 5 Update 1  Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications 5 Update 1  Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications 5 Update 1  Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications 5 Update 1  Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications 5 Update 1  Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications\n",
      "Question: What is the purpose of the NVIDIA CUDA Toolkit?\n",
      "Answer: to provide guidelines for obtaining the best performance from NVIDIA GPUs\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "question = input(\"Enter your question: \")\n",
    "if question:\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=13\n",
    "    )\n",
    "    retrieved_docs = \" \".join([doc for sublist in results['documents'] for doc in sublist])\n",
    "    print(\"Response Form Database:\", retrieved_docs)\n",
    "    \n",
    "    inputs = tokenizer(question, retrieved_docs, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    \n",
    "    answer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end], skip_special_tokens=True)\n",
    "    answer = answer.replace(question, '').strip()\n",
    "    if len(answer) == 0:\n",
    "        answer = (responses[random.randint(0, len(responses)-1)])\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the purpose of the NVIDIA CUDA Toolkit?\n",
    "# What does the CUDA Toolkit End User License Agreement (EULA) cover?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
